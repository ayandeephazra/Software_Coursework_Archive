Vectors are collections of numbers arranged as a row or column while matrices are collections of values arranged in rows and columns. Supervised learning enables the use of matrices for linear classifiers. Linear classifiers solve systems of linear equations to categorize data. Data-modelling is done by multiplying a feature vector and weight vector to get outer product. Multiple taste profiles added as new inner products sums can give more accurate results for ratings profiles. Due to noise and other limitations, machine learning problems often do not have unique solutions. Therefore, to solve ML problems, we must find approximate solutions. To get the best approximate solutions for an overdetermined system, we have to minimize the squared error of a function. If we capture noise in our problem’s solution, we might overfit the data and not be able to generalize the solution. For solving underdetermined systems, we use ridge regression. Low-rank decompositions emphasize patterns in a matrix, help complete missing data using low-rank approximations, and help mitigate isotropic noise in data. Patterned data is placed into ‘K’ clusters by using the K-Means algorithm. Singular Value Decomposition provides the best rank-r approximation to matrices, relates the principal components as left-singular vectors, and solves the classification problem using orthonormal bases which fails if problem is ill-conditioned. We must then use truncated SVD or ridge-regression to find solutions. The gradient descent method finds the minimum of cost functions while Stochastic gradient descent is computationally more efficient and optimizes functions by introducing regularization which, in turn, reduces noise. We can add constraints to the LS problem by adding a Ridge Regression or LASSO regularization term. Hinge Loss, compromises on complexity by using iterative approaches to mitigate the squared error loss during classification. It also solves non-separable data problems by implementing iterative algorithms on SVMs. A “neuron” generalizes a linear classifier and a neural network uses neurons to solve multiple problems. Backpropagation employs SGD to update the weights in each layer, starting from the last. Classification can also be done by a “kernel” which maps a linear function to a high-dimensional feature space and is used to generate curved decision boundaries, which might better fit data. Weighted kernels are summed to find a computationally efficient solution to high-dimensional regression.