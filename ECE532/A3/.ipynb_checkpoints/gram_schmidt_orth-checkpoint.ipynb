{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44721 -0.36515 -0.63246 -0.5164   0.       0.       0.       0.     ]\n",
      " [ 0.44721  0.54772  0.31623 -0.3873   0.       0.       0.       0.5    ]\n",
      " [ 0.44721 -0.36515  0.       0.6455   0.       0.       0.       0.5    ]\n",
      " [ 0.44721  0.54772 -0.31623  0.3873   0.       0.       0.      -0.5    ]\n",
      " [ 0.44721 -0.36515  0.63246 -0.1291   0.       0.       0.      -0.5    ]]\n",
      "[[0.4472136]\n",
      " [0.4472136]\n",
      " [0.4472136]\n",
      " [0.4472136]\n",
      " [0.4472136]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gram_schmidt(B):\n",
    "    \"\"\"Orthogonalize a set of vectors stored as the columns of matrix B.\"\"\"\n",
    "    # Get the number of vectors.\n",
    "    m, n = B.shape\n",
    "    # Create new matrix to hold the orthonormal basis\n",
    "    U = np.zeros([m,n]) \n",
    "    for j in range(n):\n",
    "        # To orthogonalize the vector in column j with respect to the\n",
    "        # previous vectors, subtract from it its projection onto\n",
    "        # each of the previous vectors.\n",
    "        v = B[:,j].copy()\n",
    "        for k in range(j):\n",
    "            v -= np.dot(U[:, k], B[:, j]) * U[:, k]\n",
    "        if np.linalg.norm(v)>1e-10:\n",
    "            U[:, j] = v / np.linalg.norm(v)\n",
    "    return U\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    B1 = np.array([[ 1.0,4.0,7.0,2.0,8.0,7.0,4.0,2.0],[1.0,9.0,3.0,5.0,6.0,10.0,5.0,5.0],[1.0,4.0,8.0,3.0,7.0,6.0,4.0,1.0],[1.0,9.0,2.0,6.0,5.0,9.0,5.0,4.0],[1.0,4.0,9.0,2.0,8.0,7.0,4.0,1.0]])\n",
    "    X =  np.array([[ 4.0,7.0,2.0,8.0,7.0,4.0,2.0],[9.0,3.0,5.0,6.0,10.0,5.0,5.0],[4.0,8.0,3.0,7.0,6.0,4.0,1.0],[9.0,2.0,6.0,5.0,9.0,5.0,4.0],[4.0,9.0,2.0,8.0,7.0,4.0,1.0]])\n",
    "    A1 = gram_schmidt(B1)\n",
    "    print(A1.round(5))\n",
    "    t1 = A1[:,[0]]\n",
    "    print(t1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first column is just a column of ones, on applying the first step of Gram-Schmidt orthogonalization, we obtain an orthonormal vector which normalizes the column of ones. Thus, we get t1 as defined in the question as the first column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1.T@t1 = \n",
      " [[1.]]\n",
      "\n",
      "t1.T@X = \n",
      " [[13.41641 12.96919  8.04984 15.20526 17.44133  9.8387   5.81378]]\n",
      "\n",
      " X - t1@Wrank1 = \n",
      " [[-2.   1.2 -1.6  1.2 -0.8 -0.4 -0.6]\n",
      " [ 3.  -2.8  1.4 -0.8  2.2  0.6  2.4]\n",
      " [-2.   2.2 -0.6  0.2 -1.8 -0.4 -1.6]\n",
      " [ 3.  -3.8  2.4 -1.8  1.2  0.6  1.4]\n",
      " [-2.   3.2 -1.6  1.2 -0.8 -0.4 -1.6]]\n"
     ]
    }
   ],
   "source": [
    "# rank 1 approximation\n",
    "print(\"t1.T@t1 = \\n\",t1.T@t1)\n",
    "#solution for W\n",
    "Wrank1 = t1.T@X\n",
    "print(\"\\nt1.T@X = \\n\",Wrank1.round(5))\n",
    "\n",
    "#RESIDUAL CALCULATION\n",
    "residual1 = X - t1@Wrank1\n",
    "\n",
    "print(\"\\n X - t1@Wrank1 = \\n\",residual1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t12.T@t12 = \n",
      " [[ 1. -0.]\n",
      " [-0.  1.]]\n",
      "\n",
      "t12.T@X = \n",
      " [[13.41641 12.96919  8.04984 15.20526 17.44133  9.8387   5.81378]\n",
      " [ 5.47723 -6.02495  3.46891 -2.37346  3.10376  1.09545  3.46891]]\n",
      "\n",
      " X - t12@Wrank2 = \n",
      " [[ 0.      -1.      -0.33333  0.33333  0.33333  0.       0.66667]\n",
      " [ 0.       0.5     -0.5      0.5      0.5      0.       0.5    ]\n",
      " [ 0.       0.       0.66667 -0.66667 -0.66667  0.      -0.33333]\n",
      " [ 0.      -0.5      0.5     -0.5     -0.5      0.      -0.5    ]\n",
      " [ 0.       1.      -0.33333  0.33333  0.33333  0.      -0.33333]]\n"
     ]
    }
   ],
   "source": [
    "# rank 2 approximation\n",
    "t12 = A1[:,:2]\n",
    "\n",
    "print(\"t12.T@t12 = \\n\",(t12.T@t12).round(5))\n",
    "#solution for W\n",
    "Wrank2 = t12.T@X\n",
    "print(\"\\nt12.T@X = \\n\",Wrank2.round(5))\n",
    "\n",
    "#RESIDUAL CALCULATION\n",
    "residual2 = X - t12@Wrank2\n",
    "\n",
    "print(\"\\n X - t12@Wrank2 = \\n\",residual2.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Jennifer (who's ratings are in column 2 of X) heavily prefers scifi over romantic movies, unlike Jake (who's ratings are in column 1 of X), we see that that the addition of a second column heavily alters the approximation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t123.T@t123 = \n",
      " [[ 1. -0.  0.]\n",
      " [-0.  1. -0.]\n",
      " [ 0. -0.  1.]]\n",
      "\n",
      "t123.T@X = \n",
      " [[13.41641 12.96919  8.04984 15.20526 17.44133  9.8387   5.81378]\n",
      " [ 5.47723 -6.02495  3.46891 -2.37346  3.10376  1.09545  3.46891]\n",
      " [ 0.       1.58114 -0.31623  0.31623  0.31623  0.      -0.31623]]\n",
      "\n",
      " X - t123@Wrank3 = \n",
      " [[ 0.       0.      -0.53333  0.53333  0.53333  0.       0.46667]\n",
      " [ 0.      -0.      -0.4      0.4      0.4      0.       0.6    ]\n",
      " [ 0.       0.       0.66667 -0.66667 -0.66667  0.      -0.33333]\n",
      " [ 0.       0.       0.4     -0.4     -0.4      0.      -0.6    ]\n",
      " [-0.      -0.      -0.13333  0.13333  0.13333 -0.      -0.13333]]\n"
     ]
    }
   ],
   "source": [
    "# rank 3 approximation\n",
    "t123 = A1[:,:3]\n",
    "\n",
    "print(\"t123.T@t123 = \\n\",(t123.T@t123).round(5))\n",
    "#solution for W\n",
    "Wrank3 = t123.T@X\n",
    "print(\"\\nt123.T@X = \\n\",Wrank3.round(5))\n",
    "\n",
    "#RESIDUAL CALCULATION\n",
    "residual3 = X - t123@Wrank3\n",
    "\n",
    "print(\"\\n X - t123@Wrank3 = \\n\",residual3.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as we increase the rank in the approximation, with every column that is added from the taste profile matrix, the error goes to zero in those columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t12.T@t12 = \n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "t12.T@X = \n",
      " [[12.96919 13.41641  8.04984 15.20526 17.44133  9.8387   5.81378]\n",
      " [ 6.22896 -5.29783 -3.43556  2.376   -2.92183 -1.05957 -3.43556]]\n",
      "\n",
      " X - t12@Wrank2 = \n",
      " [[ 0.      -0.97938 -0.93814  0.74227 -0.23711 -0.19588  0.06186]\n",
      " [ 0.       0.61856 -0.14433  0.26804  0.8866   0.12371  0.85567]\n",
      " [ 0.      -0.12887  0.6134  -0.63918 -0.76804 -0.02577 -0.3866 ]\n",
      " [ 0.      -0.23196  0.30412 -0.35052 -0.58247 -0.04639 -0.69588]\n",
      " [ 0.       0.72165  0.16495 -0.02062  0.70103  0.14433  0.16495]]\n",
      "\n",
      "t123.T@t123 = \n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  1. -0.]\n",
      " [ 0. -0.  1.]]\n",
      "\n",
      "t123.T@X = \n",
      " [[12.96919 13.41641  8.04984 15.20526 17.44133  9.8387   5.81378]\n",
      " [ 6.22896 -5.29783 -3.43556  2.376   -2.92183 -1.05957 -3.43556]\n",
      " [ 0.       1.39032  0.57467 -0.2966   1.09372  0.27806  0.57467]]\n",
      "\n",
      " X - t123@Wrank3 = \n",
      " [[ 0.       0.      -0.53333  0.53333  0.53333  0.       0.46667]\n",
      " [-0.      -0.      -0.4      0.4      0.4     -0.       0.6    ]\n",
      " [ 0.       0.       0.66667 -0.66667 -0.66667  0.      -0.33333]\n",
      " [ 0.       0.       0.4     -0.4     -0.4      0.      -0.6    ]\n",
      " [-0.      -0.      -0.13333  0.13333  0.13333 -0.      -0.13333]]\n"
     ]
    }
   ],
   "source": [
    "B1 = np.array([[ 1.0,7.0,4.0,2.0,8.0,7.0,4.0,2.0],[1.0,3.0,9.0,5.0,6.0,10.0,5.0,5.0],[1.0,8.0,4.0,3.0,7.0,6.0,4.0,1.0],[1.0,2.0,9.0,6.0,5.0,9.0,5.0,4.0],[1.0,9.0,4.0,2.0,8.0,7.0,4.0,1.0]])\n",
    "X =  np.array([[ 7.0,4.0,2.0,8.0,7.0,4.0,2.0],[3.0,9.0,5.0,6.0,10.0,5.0,5.0],[8.0,4.0,3.0,7.0,6.0,4.0,1.0],[2.0,9.0,6.0,5.0,9.0,5.0,4.0],[9.0,4.0,2.0,8.0,7.0,4.0,1.0]])\n",
    "A1 = gram_schmidt(B1)\n",
    "t12 = A1[:,:2]\n",
    "\n",
    "print(\"t12.T@t12 = \\n\",(t12.T@t12).round(5))\n",
    "Wrank2 = t12.T@X\n",
    "print(\"\\nt12.T@X = \\n\",Wrank2.round(5))\n",
    "\n",
    "residual2 = X - t12@Wrank2\n",
    "\n",
    "print(\"\\n X - t12@Wrank2 = \\n\",residual2.round(5))\n",
    "t123 = A1[:,:3]\n",
    "\n",
    "print(\"\\nt123.T@t123 = \\n\",(t123.T@t123).round(5))\n",
    "Wrank3 = t123.T@X\n",
    "print(\"\\nt123.T@X = \\n\",Wrank3.round(5))\n",
    "\n",
    "residual3 = X - t123@Wrank3\n",
    "\n",
    "print(\"\\n X - t123@Wrank3 = \\n\",residual3.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the rank-2 approximations are completely different to the first case where Jake and Jennifer's values were unaltered. This is because, in the Gram-Schmidt process, we first normalize the first vector and then use the results of that normalization to compute the next normal vector from the next column. Order matters here.\n",
    "\n",
    "But since only those two columns are switched, the remaining result is unaltered, and the Gram-Schmidt process is the same. Thus, the rank-3 approximation is the same in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
